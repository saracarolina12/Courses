{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six-bar Mechanism Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BetaShF import ShF\n",
    "from BetaShM import ShM \n",
    "import numpy as np \n",
    "from scipy.optimize import differential_evolution, minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from cnsg_differential_evolution import cnsg_differential_evolution\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(samples, fitness, forces, moments):\n",
    "    filterF = forces < 1\n",
    "    filterM = moments < 1\n",
    "    f = np.logical_and(filterF, filterM)\n",
    "    print(f.shape)\n",
    "    return samples[f], fitness[f], forces[f], moments[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSample(now, sample, fitness, force, moment):\n",
    "    def appendToFile(name, text): \n",
    "        with open(name, \"a\") as f:\n",
    "            f.write(text + '\\n')\n",
    "    s = \"\"\n",
    "    for x in sample: s += str(x) + \" \"\n",
    "    appendToFile(now + \"Population.txt\", s)\n",
    "    appendToFile(now + \"Fitness.txt\", str(fitness))\n",
    "    appendToFile(now + \"ShForces.txt\", str(force))\n",
    "    appendToFile(now + \"ShMoments.txt\", str(moment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-0.16m <= x_{cn},y_{cn} <= 0.16m$$\n",
    "\n",
    "$$0.005m <= t_{cn} <= 0.04m$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(s, ShF, ShM, a): #c is a constant that distributes the weight among the functions.\n",
    "    return a*ShF(s) + (1-a)*ShM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds [[-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]]\n"
     ]
    }
   ],
   "source": [
    "# Bounds for each variable\n",
    "nVar = 5\n",
    "bounds = []\n",
    "for i in range(1,nVar*3+1):\n",
    "    if(i%3==0): bounds.append([0.005,0.04])\n",
    "    else: bounds.append([-0.16, 0.16])\n",
    "bounds = np.array(bounds)\n",
    "print('bounds',bounds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_conjugado(X0,f,MaxIter=100,eps=1e-5):\n",
    "    k=0\n",
    "    X = X0\n",
    "    G = Vf(X,ShF, ShM, f)\n",
    "    normaGradiente = np.linalg.norm(G)\n",
    "    P = -G\n",
    "    curr_fit = f(X,ShF, ShM)\n",
    "    while(k<=MaxIter and normaGradiente>=eps):\n",
    "        Ap = V2fTd(X,ShF, ShM,P,f)\n",
    "        alpha = np.dot(-P,G) / np.dot(P,Ap)\n",
    "        X = X + alpha*P\n",
    "        G_ = np.copy(G)\n",
    "        G = G + alpha*Ap\n",
    "        normaGradiente = np.linalg.norm(G)\n",
    "        B = -np.dot(G,G)/np.dot(G_,G_)\n",
    "        P = -G + B*P\n",
    "        k = k+1\n",
    "        print(\"#\",k, \", fit: \", f(X,ShF, ShM))\n",
    "        if f(X,ShF, ShM) <= curr_fit:\n",
    "            best_x = X\n",
    "            curr_fit = f(best_x,ShF, ShM)\n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#################### Descenso de Gradiente con diferenciaciÃ³n finita\n",
    "eps = 1e-5\n",
    "def Gradiente(X,f, ShF, ShM):\n",
    "    n = len(X)\n",
    "    G = np.zeros((n),float)\n",
    "    incX = np.zeros((n),float)\n",
    "    for i in range(n):\n",
    "        incX[i] = eps\n",
    "        G[i] = (f(X+incX, ShF, ShM)-f(X, ShF, ShM))/eps\n",
    "        incX[i] = 0\n",
    "    return G\n",
    "def getStepSize(a,m,X,P,G,f):\n",
    "    c0 = 1e-4\n",
    "    c1 = 2\n",
    "    c2 = 5\n",
    "    c3 = 3\n",
    "    eps = 1e-8\n",
    "    alpha = a\n",
    "    while f(X+alpha*P, ShF, ShM) > f(X, ShF, ShM)+c0*np.dot(G,P):\n",
    "        m = 0\n",
    "        alpha = alpha/c1\n",
    "        if alpha<=eps:\n",
    "            break\n",
    "    m += 1\n",
    "    if m>=c2:\n",
    "        m=0\n",
    "        alpha = c3*alpha\n",
    "    return alpha,m\n",
    "\n",
    "def Gradient_Descent(X0,f,bounds,MaxIter=1000,alpha=1e-3, args=()):\n",
    "    k=0\n",
    "    X = X0\n",
    "    G = Gradiente(X,f, ShF, ShM)\n",
    "    normaGradiente = np.linalg.norm(G)\n",
    "    m = 0\n",
    "    while(k<=MaxIter and normaGradiente>=eps):\n",
    "        G = Gradiente(X,f, ShF, ShM)\n",
    "        normaGradiente = np.linalg.norm(G)\n",
    "        P = - G / normaGradiente\n",
    "        alpha,m = getStepSize(alpha,m,X,P,G,f)\n",
    "        X = X + alpha*P\n",
    "        X = np.clip(X,bounds[:,0],bounds[:,1])\n",
    "        # verficar bounds: si x() no coincide con sus bounds correspondientes\n",
    "        k = k+1\n",
    "        if k%100 == 0:\n",
    "            print(\"\\t\\t#\",k,f(X, ShF, ShM))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF: 46.604883632565645\n"
     ]
    }
   ],
   "source": [
    "s=bounds[:,0]\n",
    "r = objective_function(s, ShF, ShM,0.5)\n",
    "print('OF:',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00201097  0.13479922  0.01456812  0.14459293 -0.09138375  0.03147492\n",
      " -0.10210151  0.15345187  0.01096139  0.14256518  0.15252315  0.02462254\n",
      " -0.00616137 -0.03035997  0.0235087 ]\n"
     ]
    }
   ],
   "source": [
    "def random_start(bounds):\n",
    "    # print(bounds.shape[0])\n",
    "    arr = np.zeros(bounds.shape[0])\n",
    "    for i, tupl in enumerate(bounds):\n",
    "        rand = np.random.uniform(tupl[0], tupl[1])\n",
    "        # print(i, rand)\n",
    "        arr[i] = rand \n",
    "    return arr\n",
    "    \n",
    "print(random_start(bounds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Number of iterations: 700\n",
      "\t\n",
      "Initial Fitness: 44.84569313497913 in #0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gradient_Descent() got an unexpected keyword argument 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[39m# return fit_GD\u001b[39;00m\n\u001b[0;32m     29\u001b[0m it \u001b[39m=\u001b[39m \u001b[39m700\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[39mprint\u001b[39m(GD(\u001b[39m1e-3\u001b[39;49m, it))\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(GD(\u001b[39m1e-5\u001b[39m, it))\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(GD(\u001b[39m1e-8\u001b[39m, it))\n",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m, in \u001b[0;36mGD\u001b[1;34m(alph, iter)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mInitial Fitness: \u001b[39m\u001b[39m{\u001b[39;00mfitness\u001b[39m}\u001b[39;00m\u001b[39m in #\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> 15\u001b[0m r \u001b[39m=\u001b[39m Gradient_Descent(r,objective_function, bounds,MaxIter\u001b[39m=\u001b[39;49m\u001b[39miter\u001b[39;49m, alpha\u001b[39m=\u001b[39;49malph, args\u001b[39m=\u001b[39;49m(r, ShF, ShM, \u001b[39m0.5\u001b[39;49m))\n\u001b[0;32m     16\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m     17\u001b[0m fit_GD \u001b[39m=\u001b[39m objective_function(r,ShF,ShM, \u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Gradient_Descent() got an unexpected keyword argument 'args'"
     ]
    }
   ],
   "source": [
    "# it = 80\n",
    "# n = 4\n",
    "\n",
    "def GD(alph, iter):\n",
    "    n = 4\n",
    "    sols = np.zeros((n, 2))\n",
    "    best, bestSol = 10, None\n",
    "    eTime = 0\n",
    "    print(f'\\n* Number of iterations: {iter}')\n",
    "    for i in range(n):\n",
    "        r = random_start(bounds)\n",
    "        fitness = objective_function(r, ShF, ShM, 0.5)\n",
    "        print(f\"\\t\\nInitial Fitness: {fitness} in #{i}\")\n",
    "        start = time.perf_counter()\n",
    "        r = Gradient_Descent(r,objective_function, bounds,MaxIter=iter, alpha=alph, args=(r, ShF, ShM, 0.5))\n",
    "        end = time.perf_counter()\n",
    "        fit_GD = objective_function(r,ShF,ShM, 0.5)\n",
    "        print(\"\\t  - before: \", fitness)\n",
    "        print(\"\\t  - after (GD): \", fit_GD)\n",
    "        if fit_GD < best:\n",
    "            best = fit_GD\n",
    "            bestSol = r\n",
    "        eTime += (end-start) #Time in seconds\n",
    "        logSample(eTime, r, fitness, ShF(r), ShM(r))\n",
    "    if n: eTime /= n\n",
    "    print(\"Average time of execution:\", eTime,\"seconds. It was run\", n, \"times.\")\n",
    "    # return fit_GD\n",
    "\n",
    "it = 700\n",
    "print(GD(1e-3, it))\n",
    "print(GD(1e-5, it))\n",
    "print(GD(1e-8, it))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36d8014425df087c299dae688a2df59daa7f4b005992d2496cc7a95dceb622c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
