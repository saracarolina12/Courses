{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six-bar Mechanism Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BetaShF import ShF\n",
    "from BetaShM import ShM \n",
    "import numpy as np \n",
    "from scipy.optimize import differential_evolution, minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from cnsg_differential_evolution import cnsg_differential_evolution\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(samples, fitness, forces, moments):\n",
    "    filterF = forces < 1\n",
    "    filterM = moments < 1\n",
    "    f = np.logical_and(filterF, filterM)\n",
    "    print(f.shape)\n",
    "    return samples[f], fitness[f], forces[f], moments[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSample(now, sample, fitness, force, moment):\n",
    "    def appendToFile(name, text): \n",
    "        with open(name, \"a\") as f:\n",
    "            f.write(text + '\\n')\n",
    "    s = \"\"\n",
    "    for x in sample: s += str(x) + \" \"\n",
    "    appendToFile(now + \"Population.txt\", s)\n",
    "    appendToFile(now + \"Fitness.txt\", str(fitness))\n",
    "    appendToFile(now + \"ShForces.txt\", str(force))\n",
    "    appendToFile(now + \"ShMoments.txt\", str(moment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-0.16m <= x_{cn},y_{cn} <= 0.16m$$\n",
    "\n",
    "$$0.005m <= t_{cn} <= 0.04m$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(s, ShF, ShM, a): #c is a constant that distributes the weight among the functions.\n",
    "    return a*ShF(s) + (1-a)*ShM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds [[-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]\n",
      " [-0.16   0.16 ]\n",
      " [-0.16   0.16 ]\n",
      " [ 0.005  0.04 ]]\n"
     ]
    }
   ],
   "source": [
    "# Bounds for each variable\n",
    "nVar = 5\n",
    "bounds = []\n",
    "for i in range(1,nVar*3+1):\n",
    "    if(i%3==0): bounds.append([0.005,0.04])\n",
    "    else: bounds.append([-0.16, 0.16])\n",
    "bounds = np.array(bounds)\n",
    "print('bounds',bounds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_conjugado(X0,f,MaxIter=100,eps=1e-5):\n",
    "    k=0\n",
    "    X = X0\n",
    "    G = Vf(X,ShF, ShM, f)\n",
    "    normaGradiente = np.linalg.norm(G)\n",
    "    P = -G\n",
    "    curr_fit = f(X,ShF, ShM)\n",
    "    while(k<=MaxIter and normaGradiente>=eps):\n",
    "        Ap = V2fTd(X,ShF, ShM,P,f)\n",
    "        alpha = np.dot(-P,G) / np.dot(P,Ap)\n",
    "        X = X + alpha*P\n",
    "        G_ = np.copy(G)\n",
    "        G = G + alpha*Ap\n",
    "        normaGradiente = np.linalg.norm(G)\n",
    "        B = -np.dot(G,G)/np.dot(G_,G_)\n",
    "        P = -G + B*P\n",
    "        k = k+1\n",
    "        print(\"#\",k, \", fit: \", f(X,ShF, ShM))\n",
    "        if f(X,ShF, ShM) <= curr_fit:\n",
    "            best_x = X\n",
    "            curr_fit = f(best_x,ShF, ShM)\n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#################### Descenso de Gradiente con diferenciaciÃ³n finita\n",
    "eps = 1e-5\n",
    "def Gradiente(X,f, ShF, ShM):\n",
    "    n = len(X)\n",
    "    G = np.zeros((n),float)\n",
    "    incX = np.zeros((n),float)\n",
    "    for i in range(n):\n",
    "        incX[i] = eps\n",
    "        G[i] = (f(X+incX, ShF, ShM)-f(X, ShF, ShM))/eps\n",
    "        incX[i] = 0\n",
    "    return G\n",
    "def getStepSize(a,m,X,P,G,f):\n",
    "    c0 = 1e-4\n",
    "    c1 = 2\n",
    "    c2 = 5\n",
    "    c3 = 3\n",
    "    eps = 1e-8\n",
    "    alpha = a\n",
    "    while f(X+alpha*P, ShF, ShM) > f(X, ShF, ShM)+c0*np.dot(G,P):\n",
    "        m = 0\n",
    "        alpha = alpha/c1\n",
    "        if alpha<=eps:\n",
    "            break\n",
    "    m += 1\n",
    "    if m>=c2:\n",
    "        m=0\n",
    "        alpha = c3*alpha\n",
    "    return alpha,m\n",
    "\n",
    "def Gradient_Descent(X0,f,bounds,MaxIter=1000,alpha=1e-3):\n",
    "    k=0\n",
    "    X = X0\n",
    "    G = Gradiente(X,f, ShF, ShM)\n",
    "    normaGradiente = np.linalg.norm(G)\n",
    "    m = 0\n",
    "    while(k<=MaxIter and normaGradiente>=eps):\n",
    "        G = Gradiente(X,f, ShF, ShM)\n",
    "        normaGradiente = np.linalg.norm(G)\n",
    "        P = - G / normaGradiente\n",
    "        alpha,m = getStepSize(alpha,m,X,P,G,f)\n",
    "        X = X + alpha*P\n",
    "        X = np.clip(X,bounds[:,0],bounds[:,1])\n",
    "        # verficar bounds: si x() no coincide con sus bounds correspondientes\n",
    "        k = k+1\n",
    "        if k%100 == 0:\n",
    "            print(\"\\t\\t#\",k,f(X, ShF, ShM))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF: 46.604883632565645\n"
     ]
    }
   ],
   "source": [
    "s=bounds[:,0]\n",
    "r = objective_function(s, ShF, ShM,0.5)\n",
    "print('OF:',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00052678 -0.09033851  0.00938731  0.06676138  0.09533738  0.0320544\n",
      " -0.10050153  0.11142163  0.00951557 -0.09210145  0.1485086   0.03725161\n",
      " -0.07886071 -0.09890708  0.01401429]\n"
     ]
    }
   ],
   "source": [
    "def random_start(bounds):\n",
    "    # print(bounds.shape[0])\n",
    "    arr = np.zeros(bounds.shape[0])\n",
    "    for i, tupl in enumerate(bounds):\n",
    "        rand = np.random.uniform(tupl[0], tupl[1])\n",
    "        # print(i, rand)\n",
    "        arr[i] = rand \n",
    "    return arr\n",
    "    \n",
    "print(random_start(bounds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Number of iterations: 700\n",
      "\t\n",
      "Initial Fitness: 22.56949888982971 in #0\n",
      "\t\t# 100 0.5599638725626395\n",
      "\t\t# 200 0.4955442996748294\n",
      "\t\t# 300 0.4945356349259049\n",
      "\t\t# 400 0.4946503741589774\n",
      "\t\t# 500 0.49570697898223576\n",
      "\t\t# 600 0.49445754795343955\n",
      "\t\t# 700 0.49492465004120734\n",
      "\t  - before:  22.56949888982971\n",
      "\t  - after (GD):  0.49470318195097307\n",
      "\t\n",
      "Initial Fitness: 47.59492041553046 in #1\n",
      "\t\t# 100 0.6084312705358962\n",
      "\t\t# 200 0.49751771546159956\n",
      "\t\t# 300 0.49573106819531654\n",
      "\t\t# 400 0.4944346481723204\n",
      "\t\t# 500 0.49472931303520395\n",
      "\t\t# 600 0.49455973942652737\n",
      "\t\t# 700 0.4944349955933619\n",
      "\t  - before:  47.59492041553046\n",
      "\t  - after (GD):  0.4944429173428567\n",
      "\t\n",
      "Initial Fitness: 18.189827936872323 in #2\n",
      "\t\t# 100 0.6454546267054095\n",
      "\t\t# 200 0.5127697908245381\n",
      "\t\t# 300 0.4946284797863457\n",
      "\t\t# 400 0.4945968471171239\n",
      "\t\t# 500 0.49469326915338674\n",
      "\t\t# 600 0.49442428487387124\n",
      "\t\t# 700 0.4944256982854667\n",
      "\t  - before:  18.189827936872323\n",
      "\t  - after (GD):  0.4944252270295144\n",
      "\t\n",
      "Initial Fitness: 43.37956367522239 in #3\n",
      "\t\t# 100 0.7944887250187396\n",
      "\t\t# 200 0.7394288862020097\n",
      "\t\t# 300 0.7394288862020097\n",
      "\t\t# 400 0.7394288862020097\n",
      "\t\t# 500 0.7394288862020097\n",
      "\t\t# 600 0.7394288862020097\n",
      "\t\t# 700 0.7394288862020097\n",
      "\t  - before:  43.37956367522239\n",
      "\t  - after (GD):  0.7394288862020097\n",
      "Average time of execution: 322.6716251500011 seconds. It was run 4 times.\n",
      "None\n",
      "\n",
      "* Number of iterations: 700\n",
      "\t\n",
      "Initial Fitness: 48.64337179878355 in #0\n",
      "\t\t# 100 48.643334633615595\n",
      "\t\t# 200 48.643334633615595\n",
      "\t\t# 300 48.643334633615595\n",
      "\t\t# 400 48.643334633615595\n",
      "\t\t# 500 48.643334633615595\n",
      "\t\t# 600 48.643334633615595\n",
      "\t\t# 700 48.643334633615595\n",
      "\t  - before:  48.64337179878355\n",
      "\t  - after (GD):  48.643334633615595\n",
      "\t\n",
      "Initial Fitness: 32.373173444506136 in #1\n",
      "\t\t# 100 32.37315415735376\n",
      "\t\t# 200 32.37315415735376\n",
      "\t\t# 300 32.37315415735376\n",
      "\t\t# 400 32.37315415735376\n",
      "\t\t# 500 32.37315415735376\n",
      "\t\t# 600 32.37315415735376\n",
      "\t\t# 700 32.37315415735376\n",
      "\t  - before:  32.373173444506136\n",
      "\t  - after (GD):  32.37315415735376\n",
      "\t\n",
      "Initial Fitness: 48.761785530194544 in #2\n",
      "\t\t# 100 48.7617585401004\n",
      "\t\t# 200 48.7617585401004\n",
      "\t\t# 300 48.7617585401004\n",
      "\t\t# 400 48.7617585401004\n",
      "\t\t# 500 48.7617585401004\n",
      "\t\t# 600 48.7617585401004\n",
      "\t\t# 700 48.7617585401004\n",
      "\t  - before:  48.761785530194544\n",
      "\t  - after (GD):  48.7617585401004\n",
      "\t\n",
      "Initial Fitness: 32.74171348801959 in #3\n",
      "\t\t# 100 32.741698517391896\n",
      "\t\t# 200 32.741698517391896\n",
      "\t\t# 300 32.741698517391896\n",
      "\t\t# 400 32.741698517391896\n",
      "\t\t# 500 32.741698517391896\n",
      "\t\t# 600 32.741698517391896\n",
      "\t\t# 700 32.741698517391896\n",
      "\t  - before:  32.74171348801959\n",
      "\t  - after (GD):  32.741698517391896\n",
      "Average time of execution: 304.23249812500126 seconds. It was run 4 times.\n",
      "None\n",
      "\n",
      "* Number of iterations: 700\n",
      "\t\n",
      "Initial Fitness: 26.573589628611668 in #0\n",
      "\t\t# 100 26.573576380990644\n",
      "\t\t# 200 26.573576380990644\n",
      "\t\t# 300 26.573576380990644\n",
      "\t\t# 400 26.573576380990644\n",
      "\t\t# 500 26.573576380990644\n",
      "\t\t# 600 26.573576380990644\n",
      "\t\t# 700 26.573576380990644\n",
      "\t  - before:  26.573589628611668\n",
      "\t  - after (GD):  26.573576380990644\n",
      "\t\n",
      "Initial Fitness: 16.824038718941054 in #1\n",
      "\t\t# 100 16.824033976699653\n",
      "\t\t# 200 16.824033976699653\n",
      "\t\t# 300 16.824033976699653\n",
      "\t\t# 400 16.824033976699653\n",
      "\t\t# 500 16.824033976699653\n",
      "\t\t# 600 16.824033976699653\n",
      "\t\t# 700 16.824033976699653\n",
      "\t  - before:  16.824038718941054\n",
      "\t  - after (GD):  16.824033976699653\n",
      "\t\n",
      "Initial Fitness: 76.43334793154713 in #2\n",
      "\t\t# 100 76.43332583565333\n",
      "\t\t# 200 76.43332583565333\n",
      "\t\t# 300 76.43332583565333\n",
      "\t\t# 400 76.43332583565333\n",
      "\t\t# 500 76.43332583565333\n",
      "\t\t# 600 76.43332583565333\n",
      "\t\t# 700 76.43332583565333\n",
      "\t  - before:  76.43334793154713\n",
      "\t  - after (GD):  76.43332583565333\n",
      "\t\n",
      "Initial Fitness: 34.4598576580508 in #3\n",
      "\t\t# 100 34.45984795591551\n",
      "\t\t# 200 34.45984795591551\n",
      "\t\t# 300 34.45984795591551\n",
      "\t\t# 400 34.45984795591551\n",
      "\t\t# 500 34.45984795591551\n",
      "\t\t# 600 34.45984795591551\n",
      "\t\t# 700 34.45984795591551\n",
      "\t  - before:  34.4598576580508\n",
      "\t  - after (GD):  34.45984795591551\n",
      "Average time of execution: 292.79278052500104 seconds. It was run 4 times.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# it = 80\n",
    "# n = 4\n",
    "\n",
    "def GD(alph, iter):\n",
    "    n = 4\n",
    "    sols = np.zeros((n, 2))\n",
    "    best, bestSol = 10, None\n",
    "    eTime = 0\n",
    "    print(f'\\n* Number of iterations: {iter}')\n",
    "    for i in range(n):\n",
    "        r = random_start(bounds)\n",
    "        fitness = objective_function(r, ShF, ShM, 0.5)\n",
    "        print(f\"\\t\\nInitial Fitness: {fitness} in #{i}\")\n",
    "        start = time.perf_counter()\n",
    "        r = Gradient_Descent(r,objective_function, bounds,MaxIter=iter, alpha=alph)\n",
    "        end = time.perf_counter()\n",
    "        fit_GD = objective_function(r,ShF,ShM, 0.5)\n",
    "        print(\"\\t  - before: \", fitness)\n",
    "        print(\"\\t  - after (GD): \", fit_GD)\n",
    "        if fit_GD < best:\n",
    "            best = fit_GD\n",
    "            bestSol = r\n",
    "        eTime += (end-start) #Time in seconds\n",
    "    if n: eTime /= n\n",
    "    print(\"Average time of execution:\", eTime,\"seconds. It was run\", n, \"times.\")\n",
    "    # return fit_GD\n",
    "\n",
    "it = 700\n",
    "print(GD(1e-3, it))\n",
    "print(GD(1e-5, it))\n",
    "print(GD(1e-8, it))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36d8014425df087c299dae688a2df59daa7f4b005992d2496cc7a95dceb622c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
